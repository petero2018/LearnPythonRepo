{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "import csv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "        .master('local') \\\n",
    "        .appName('First Spark app for testing') \\\n",
    "        .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "print('Spark version: ',spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('/home/peter/Downloads/LastFM/songs1.tsv',header=True, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of actual dtypes to check\n",
    "actual_dtypes_list = df.dtypes\n",
    "print(actual_dtypes_list)\n",
    "\n",
    "# Iterate through the list of actual dtypes tuples\n",
    "for attribute_tuple in actual_dtypes_list:\n",
    "  \n",
    "  # Check if column name is dictionary of expected dtypes\n",
    "    col_name = attribute_tuple[0]\n",
    "      if col_name in validation_dict.keys():\n",
    "\n",
    "    # Compare attribute types\n",
    "    col_type = attribute_tuple[1]\n",
    "    if col_type == validation_dict[col_name]:\n",
    "        print(col_name + ' has expected dtype.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name and value of col with max corr\n",
    "corr_max = 0\n",
    "corr_max_col = columns[0]\n",
    "\n",
    "# Loop to check all columns contained in list\n",
    "for col in columns:\n",
    "    # Check the correlation of a pair of columns\n",
    "    corr_val = df.corr(col, 'SALESCLOSEPRICE')\n",
    "    # Logic to compare corr_max with current corr_val\n",
    "    if corr_val > corr_max:\n",
    "        # Update the column name and corr value\n",
    "        corr_max = corr_val\n",
    "        corr_max_col = col\n",
    "\n",
    "print(corr_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a single column and sample and convert to pandas\n",
    "sample_df = df.select(['LISTPRICE']).sample(False, 0.5, 42)\n",
    "pandas_df = sample_df.toPandas()\n",
    "\n",
    "# Plot distribution of pandas_df and display plot\n",
    "sns.distplot(pandas_df)\n",
    "plt.show()\n",
    "\n",
    "# Import skewness function\n",
    "from pyspark.sql.functions import skewness\n",
    "\n",
    "# Compute and print skewness of LISTPRICE\n",
    "print(df.agg({'LISTPRICE': 'skewness'}).collect())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
