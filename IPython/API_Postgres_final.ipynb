{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests,json\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection Test..\n",
      "('PostgreSQL 10.6 (Ubuntu 10.6-0ubuntu0.18.04.1) on x86_64-pc-linux-gnu, compiled by gcc (Ubuntu 7.3.0-16ubuntu3) 7.3.0, 64-bit',)\n"
     ]
    }
   ],
   "source": [
    "engine = create_engine('postgresql://petero:test123@localhost:5432/API')\n",
    "with engine.connect() as con: \n",
    "    rs = con.execute('SELECT version()')\n",
    "    for row in rs:\n",
    "        print('Connection Test..')\n",
    "        print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pages:  7 records_per_page:  50 total_records:  304\n"
     ]
    }
   ],
   "source": [
    "rawdata = requests.get('http://api.worldbank.org/v2/country?format=json')\n",
    "data = rawdata.json()\n",
    "#data[0]['pages'] # you need to do a get request to figure out the number of pages which could change\n",
    "pages, per_page, total = data[0]['pages'], data[0]['per_page'], data[0]['total']\n",
    "print('pages: ',pages, 'records_per_page: ',per_page, 'total_records: ',total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following files have been removed: /home/peter/Work/Mudano/2019-03-10_API_Data.json\n",
      "Page 1 has been loaded\n",
      "Page 1 has been dumped to JSON file\n",
      "Page 1 has been loaded to the API.public.API_Data\n",
      "Page 2 has been loaded\n",
      "Page 2 has been dumped to JSON file\n",
      "Page 2 has been loaded to the API.public.API_Data\n",
      "Page 3 has been loaded\n",
      "Page 3 has been dumped to JSON file\n",
      "Page 3 has been loaded to the API.public.API_Data\n",
      "Page 4 has been loaded\n",
      "Page 4 has been dumped to JSON file\n",
      "Page 4 has been loaded to the API.public.API_Data\n",
      "Page 5 has been loaded\n",
      "Page 5 has been dumped to JSON file\n",
      "Page 5 has been loaded to the API.public.API_Data\n",
      "Page 6 has been loaded\n",
      "Page 6 has been dumped to JSON file\n",
      "Page 6 has been loaded to the API.public.API_Data\n",
      "Page 7 has been loaded\n",
      "Page 7 has been dumped to JSON file\n",
      "Page 7 has been loaded to the API.public.API_Data\n"
     ]
    }
   ],
   "source": [
    "JsonFile = '/home/peter/Work/Mudano/'+str(datetime.date.today())+'_API_Data.json'\n",
    "\n",
    "if os.path.exists(JsonFile):\n",
    "    \n",
    "    try:\n",
    "        os.remove(JsonFile)\n",
    "        print('The following files have been removed: '+JsonFile)\n",
    "    except:\n",
    "        print(\"Error while deleting file \", JsonFile)\n",
    "        \n",
    "Session = sessionmaker(bind=engine)\n",
    "\n",
    "if engine.dialect.has_table(engine, 'API_Data'):\n",
    "    session1 = Session()\n",
    "    session1.execute('''TRUNCATE TABLE \"API\".\"public\".\"API_Data\"''')\n",
    "    session1.commit()\n",
    "    \n",
    "    session1.close()\n",
    "\n",
    "if engine.dialect.has_table(engine, 'CSV_Data'):\n",
    "    session2 = Session()\n",
    "    session2.execute('''TRUNCATE TABLE \"API\".\"public\".\"CSV_Data\"''')\n",
    "    session2.commit()\n",
    "    \n",
    "    session2.close()\n",
    "\n",
    "for pageNo in [i+1 for i in range(data[0]['pages'])]:\n",
    "    payload = {'page': pageNo}\n",
    "    rawdata = requests.get('http://api.worldbank.org/v2/country?format=json', params=payload)\n",
    "    print('Page '+str(pageNo)+' has been loaded')\n",
    "    \n",
    "    #keep a copy on the disc\n",
    "    with open(JsonFile,'a') as f:\n",
    "        json.dump(rawdata.json()[1],f, indent=2)\n",
    "        print('Page '+str(pageNo)+' has been dumped to JSON file')\n",
    "    \n",
    "    #flatten the data and insert it to \"API\".\"public\".\"API_Data\"\n",
    "    df1 = pd.io.json.json_normalize(rawdata.json()[1])\n",
    "    df1 = df1.rename(columns={'adminregion.id':'AdminRegion_ID','adminregion.iso2code':'AdminRegion_ISO2Code',\\\n",
    "                          'latitude':'Latitude','longitude':'Longtitude',\\\n",
    "                          'adminregion.value':'AdminRegion','capitalCity':'CapitalCity','id':'Country_ID',\\\n",
    "                          'incomeLevel.id':'IncomeLevel_ID','incomeLevel.iso2code':'IncomeLevel_ISO2Code',\\\n",
    "                          'incomeLevel.value':'IncomeLevel','iso2Code':'Country_ISO2Code',\\\n",
    "                          'lendingType.id':'LendinType_ID', 'lendingType.iso2code':'LendingType_ISO2Code',\\\n",
    "                          'lendingType.value':'LendinType', 'name':'CountryName', 'region.id':'Region_ID',\\\n",
    "                          'region.iso2code':'Region_ISO2Code','region.value':'Region'\n",
    "                         })\n",
    "    df1.to_sql(name='API_Data',con=engine,if_exists='append')\n",
    "    print('Page '+str(pageNo)+' has been loaded to the API.public.API_Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV_Data has been loaded to API.public.CSV_Data\n"
     ]
    }
   ],
   "source": [
    "CSV_File = '/home/peter/Work/Mudano/GEPData.csv'\n",
    "df2 = pd.read_csv(CSV_File)\n",
    "df2 = df2.drop(['1999','2000','2001','2002','2003','2004','2005','2006','2007','2008','2009','2010','2011','2012'\\\n",
    "                ,'2013','2014','2015', 'Unnamed: 27'\n",
    "               ],axis=1)\n",
    "df2.to_sql(name='CSV_Data',con=engine,if_exists='replace')\n",
    "print('CSV_Data has been loaded to API.public.CSV_Data')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
